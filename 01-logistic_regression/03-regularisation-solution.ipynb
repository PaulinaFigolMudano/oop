{"metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.6"}}, "nbformat": 4, "nbformat_minor": 2, "cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Logistic Regression and Regularisation"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#Import libraries\n", "import pandas as pd\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "from sklearn.preprocessing import LabelEncoder\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.metrics import accuracy_score\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Loading the data\n", "\n", "For this example we use a smaller dataset in the interest of CPU time. The dataset comes from [the UCI repository](https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/) and is data from observed tissues in a breast cancer study. \n", "\n", "We won't look in details at what the features are but each column corresponds to some measurement of interest to the study. \n", "\n", "Note that some columns record the standard deviation over measurements and may therefore be highly correlated with other columns (*why is that a bad thing?*) we will come back to that in the regularisation part.\n", "\n", "Load `data/breastcancerdata.csv` and have a look at it writing `data.head()`.\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# add your code here to import and inspect the data ...\n", "data = pd.read_csv(\"data/breastcancerdata.csv\", header=None)\n", "data.head()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Extracting the response\n", "\n", "The first column are IDs corresponding to the patient, we will therefore ignore that column. The second column is the response of interest with `M` (malignant) and `B` (benign). You need to tell Sklearn that these are the two classes of interest. One way of doing this is to use the `LabelEncoder` tool from `sklearn.preprocessing`. In the below cell we:\n", "\n", "* create a LabelEncoder and call it `le`\n", "* fit the encoder to the unique values of that column using `loc` to specify the column and `unique` to find the unique values\n", "* apply the label encoder by using `le.transform` on the column, name the result `response`"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["le = LabelEncoder()\n", "le.fit(data.loc[:, 1].unique())\n", "response = le.transform(data.loc[:, 1])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You can check what `response` looks like by outputting the first few values with \n", "\n", "```python\n", "print(response[0:20])\n", "```\n", "\n", "the result you should get is\n", "\n", "> `[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0]`\n", "\n", "### Extracting the feature matrix\n", "\n", "The feature matrix is constituted of the remaining columns. Using `loc` again, extract those columns and store them as `featmatrix`.\n", "\n", "The resulting matrix should have `30` columns (you can check that with `featmatrix.shape` which should return `(569,30)`)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# add your code here...\n", "featmatrix = data.loc[:, 2:]\n", "featmatrix.head()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Train Test Split\n", "\n", "Now that we have the feature matrix and the response, split it using the `train_test_split` function from `sklearn.model_selection`. \n", "Name the results `X_train, X_test, y_train, y_test`. Use `random_state=321` for reproducibility of the results."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# add your code here...\n", "X_train, X_test, y_train, y_test = train_test_split(featmatrix, response, \n", "                                                    random_state=321)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Applying the Logistic Regression\n", "\n", "The `LogisticRegression` model is located in `sklearn.linear_model` ([sklearn documentation here](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)). \n", "Define a default model, fit it on the training data and predict `X_test`. Using `accuracy_score` from `sklearn.metrics`, display the accuracy.\n", "\n", "**Note** `.predict` will return a class (0/1) while `.predict_proba` will return the score (in $[0, 1]$). "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# add your code here...\n", "\n", "logreg = LogisticRegression()\n", "logreg.fit(X_train,y_train)\n", "ypred_lr = logreg.predict(X_test)\n", "\n", "print(\"Accuracy on the test set: {0:.3f}\".format(accuracy_score(ypred_lr, y_test)))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Regularisation\n", "\n", "## Motivation\n", "\n", "When some features are almost colinear, meaning that a feature could be expressed with high accuracy by a linear combination of other features, the resulting model can be unstable and generalise badly. \n", "\n", "A simple way to test for colinearity is to compute the condition number of the feature matrix (this is relatively cheap when $\\min(n, p)$ is less than 1000). \n", "\n", "The condition number is given by the ratio of the largest singular value to the smallest singular value. \n", "If it is very large, it means that the first principal component carries significantly more information (variance) than the last principal component. \n", "In other words, it hints at the fact that the last few components add very little information and therefore that the *effective dimensionality* of the dataset is less than $p$.\n", "\n", "A condition number much higher than 1000 is typically a sign of colinearity and a hint that either PCA should be applied or regularisation. \n", "\n", "1. Compute the condition number of the feature matrix using `np.linalg.cond` and discuss"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# add your code here to compute the condition number both ways\n", "print(\"Condition number from cond:    {0:.1f}\".format(np.linalg.cond(featmatrix)))\n", "\n", "# BONUS: condition number is related to the SVD...\n", "_, S, _ = np.linalg.svd(featmatrix)\n", "print(\"Condition number from the SVD: {0:.1f}\".format(np.max(S) / np.min(S)))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Why does colinearity matter? \n", "\n", "Go backwards a bit in your notebook, at the place where you did the `train_test_split`. \n", "Change the random state from `321` to `123` and re-run the following few cells. \n", "What is the accuracy now? \n", "What does that tell you? \n", "\n", "After having had a look at that, set the random state back to `321` and re-execute the cells (so that we're all on the same page).\n", "\n", "## Adding regularisation to the logistic regression\n", "\n", "With SkLearn it's very easy to specify a regularisation term (`l1` or `l2`) as well as the strength of the regularisation `C`. \n", "\n", "To see this, define a logistic regression as before but this time, instead of just using `LogisticRegression()`, specify the `penalty` as being `l1` and specify `C` to be `2.0` (which corresponds to $\\lambda = 0.5$). \n", "Fit it, predict the test and show the accuracy.\n", "\n", "**Note**: the `C` parameter is the **inverse** of the regularisation strength. In other words, **lower** `C` means **more** regularisation."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# add your code here\n", "logreg_reg = LogisticRegression(penalty='l1', C=2.0)\n", "logreg_reg.fit(X_train, y_train)\n", "ypred_reg = logreg_reg.predict(X_test)\n", "accuracy_score(ypred_reg, y_test)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## (Bonus) having a look at the coefficients\n", "\n", "We will discuss this in more details in a minute but if you've arleady reached this point, you can try getting an intuition:\n", "\n", "* display the coefficients for the basic logistic regression\n", "* display the coefficients for the logistic regression with l1 regularisation (obtained before with grid search)\n", "\n", "What do you observe? and why do you think that's a desirable trait of l1 regularisation?\n", "\n", "**Note**: here you may want to use `plt.stem` to display the coefficients.\n", "Do scale the `y-axis` so that the amplitudes can be compared. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# your code here\n", "logreg_coefs = logreg.coef_[0]\n", "logreg_reg_coefs = logreg_reg.coef_[0]\n", "\n", "plt.figure(figsize=(12, 5))\n", "\n", "plt.subplot(121)\n", "plt.stem(np.arange(0,len(logreg_coefs)), logreg_coefs)\n", "plt.xlabel(\"Index of coefficient\", fontsize=12)\n", "plt.ylabel(\"Magnitude of coefficient\", fontsize=12)\n", "plt.ylim([-3.5, 3.5])\n", "\n", "plt.subplot(122)\n", "plt.stem(np.arange(0,len(logreg_reg_coefs)), logreg_reg_coefs)\n", "plt.xlabel(\"Index of coefficient\", fontsize=12)\n", "plt.ylim([-3.5, 3.5])\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## (Bonus) Using cross validation to find the best hyperparameter value"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Ok so now we'd like to test a range of `C` for both `l1` and `l2` in order to find the \"best\" set of parameters, of course `GridSearchCV` is here to help! Note that you could also use `LogisticRegressionCV` which contains its own cross validation tool. \n", "\n", "Take the penalty to be either `l1` or `l2` and `C` to be $2^{-5}, 2^{-4}, \\dots, 2^4, 2^5$. (It is standard to use regularisation strengths on a logarithmic scale)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# your code here\n", "from sklearn.model_selection import GridSearchCV\n", "\n", "# for each parameter specify a range of values you want to test\n", "penalty    = ['l1', 'l2']\n", "C_range    = 2. ** np.arange(-5, 5, step=1)\n", "# build a dictionary of parameters\n", "parameters = [{'C': C_range, 'penalty': penalty}]\n", "\n", "# pass the dictionary to GridSearchCV specifying that it's the LogisticRegression\n", "# and indicating the number of cross validation folds\n", "grid = GridSearchCV(LogisticRegression(), parameters, cv=5)\n", "grid.fit(X_train, y_train)\n", "\n", "# display the results\n", "bestC = grid.best_params_['C']\n", "bestP = grid.best_params_['penalty']\n", "print (\"The best parameters are: cost=\", bestC , \" and penalty=\", bestP, \"\\n\")\n", "\n", "# use the best parameters and check the accuracy\n", "print(\"Accuracy: {0:.4f}\".format(accuracy_score(y_test, grid.predict(X_test))))\n"]}]}