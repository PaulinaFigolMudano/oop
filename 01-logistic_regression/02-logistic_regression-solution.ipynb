{"metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.6"}}, "nbformat": 4, "nbformat_minor": 2, "cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Logistic Regression from Scratch"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Import any required modules here\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In this notebook we will be coding up Logistic Regression from scratch. We're doing this for two main reasons:\n", "1. To make sure we understand the model fully\n", "1. The step from this to neural networks is small\n", "\n", "Once you've mastered coding up and this kind of model, and fitting its parameters with gradient descent, all the fancy neural network literature will be much more accessible."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## The sigmoid function"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Firstly, lets get to grips with the sigmoid function. Recall: linear regression model took a D dimensional input and produced a single number, the prediction. This number could be any value on the real line. We can alter this function to restrict the output to be between 0 and 1 by using a sigmoid function. i.e. $f(x) = w_0*x_0 + w_1*x_1$ can become $f(x) = \\sigma(w_0*x_0 + w_1*x_1)$."]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Create a sigmoid function called `sig` using numpy and plot it with inputs ranging from -10 to 10"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def sig(x):\n", "    output = 1/(1 + np.exp(-x))\n", "    return output\n", "\n", "xx = np.linspace(-10, 10, 100)\n", "plt.plot(xx, sig(xx))\n", "plt.xlabel('$x$')\n", "plt.ylabel('$\\sigma(x)$')\n", "plt.grid(True)\n", "plt.show()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["If the 'linear regression equation' (i.e. the input to the sigmoid) returns a value <0, the output is <0.5. We could use a cutoff of 0.5 and return 'Class 0' for all inputs with an output of <0.5, and 'Class 1' for all inputs with an output of >=0.5. \n", "\n", "This is the inspiration for the Logistic Regression model: it returns a value between 0 and 1 which we can interpret as the probability of being in class 1."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Creating the model"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We're now going to create a function which performs the logistic regression on some dummy data. We're then going to visualise the result."]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Create a function `logistic_regression`\n", "which takes as input:\n", "* the data `X` - an NxD dimensional matrix of the data (N datapoints)\n", "* a vector `w` - a D dimensional vector of the parameters\n", "\n", "and returns:\n", "* `p` - an N dimensional output of predicted probabilities\n", "\n", "(you can use your `sig` function defined above)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def logistic_regression(X, w):\n", "    a = X @ w\n", "    p = sig(a)\n", "    return p\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Dummy data"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Run the below cell to create some dummy data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["np.random.seed(1337)\n", "nr_zero = 10\n", "nr_one = 20\n", "X = np.vstack(\n", "    [np.random.standard_normal((nr_zero, 2)),\n", "     np.random.standard_normal((nr_one, 2))*.5 + [2, 2]]\n", ")\n", "y = np.hstack([np.zeros(nr_zero), np.ones(nr_one)])\n", "plt.scatter(X[y==0, 0], X[y==0, 1], marker='o', label='class 0')\n", "plt.scatter(X[y==1, 0], X[y==1, 1], marker='x', label='class 1')\n", "plt.xlabel('$x_1$')\n", "plt.ylabel('$x_2$')\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Plot the output of `logistic_regression` with varying `w`\n", "Use the code given below to plot the probabilities predicted by different parameter settings `w`. Try:\n", "* [0, 0] (what class would this predict everywhere?)\n", "* [0, 1]\n", "* [1, 0]\n", "* [-1, 1]\n", "* [1, 1]\n", "* [1, 2]\n", "* [1, 3]\n", "* [10, 30]\n", "* [.1, .3]\n", "\n", "Comment on what you notice."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def plot_logistic_probs(w, X, y):\n", "    \"\"\"\n", "    Plots the probability surface for a 2d logistic regression\n", "    with parameters w, with data X, y\n", "    \"\"\"\n", "    plt.scatter(X[y==0, -2], X[y==0, -1], marker='o', \n", "                color='k', edgecolors='w', label='class 0')\n", "    plt.scatter(X[y==1, -2], X[y==1, -1], marker='X', \n", "                color='k', edgecolors='w', label='class 1')\n", "    plt.xlabel('$x_1$')\n", "    plt.ylabel('$x_2$')\n", "    ax = plt.gca()\n", "    x1_range = ax.get_xlim()\n", "    x2_range = ax.get_ylim()\n", "    xx1 = np.linspace(*x1_range, 100)\n", "    xx2 = np.linspace(*x2_range, 100)\n", "    xx1, xx2 = np.meshgrid(xx1, xx2)\n", "    X = np.c_[xx1.ravel(), xx2.ravel()]\n", "    if len(w) == 3:\n", "        X = np.hstack([np.ones([X.shape[0], 1]), X])\n", "        b, w_1, w_2 = w\n", "    else:\n", "        b, w_1, w_2 = np.hstack(([0], w))\n", "    p = logistic_regression(X, w)\n", "    plt.imshow(p.reshape(100, 100), vmin=0, vmax=1, \n", "               extent=x1_range+x2_range, origin='lower', cmap='bwr')\n", "    plt.colorbar()\n", "    xx1 = np.linspace(*x1_range, 100)\n", "    m = -w_1/w_2\n", "    c = -b/w_2 \n", "    xx2 = m*xx1 + c\n", "    mask = (xx2<x2_range[1]) & (xx2>x2_range[0])\n", "    if sum(mask) > 0:\n", "        plt.plot(xx1[mask], xx2[mask], '--', label='decision boundary')\n", "    plt.legend(loc='best')\n", "    \n", "plot_logistic_probs([-3, 1, 1], X, y)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# run plot_logistic_probs with various values of w\n", "# plot_logistic_probs([0, 0], X, y)\n", "# plot_logistic_probs([0, 1], X, y)\n", "# plot_logistic_probs([1, 0], X, y)\n", "# plot_logistic_probs([-1, 1], X, y)\n", "# plot_logistic_probs([1, 1], X, y)\n", "# plot_logistic_probs([1, 2], X, y)\n", "# plot_logistic_probs([1, 3], X, y)\n", "# plot_logistic_probs([10, 30], X, y)\n", "plot_logistic_probs([.1, .3], X, y)\n", "# plot_logistic_probs([-1.8, 1, 1.1], X, y)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# What did you find? See solutions for comments\n", "# * All the decision boundaries (i.e. the line with p=0.5) pass through the origin (0, 0)\n", "#     * we have not coded in a bias term yet\n", "#     * the plotting function is written such that you can provide a size 3 weight vector\n", "#     * by eye, plot_logistic_probs([-1.8, 1, 1.1], X, y) looks a pretty good fit\n", "# * The larger the parameters, the 'sharper' the boundary\n", "# * The slope of the decision boundary is the negative ratio of the parameters w\n", "#     * i.e. if you multiply w by a constant, the boundary is the same\n", "#     * (aside) it's actually -w_1/w_2 (see function for the calculation)\n", "#     * (aside) great math exercise to derive! Start with sigma(z) = 0.5\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Fitting the model"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now we have played with the model and (maybe) tried to fit the parameters `w` ourselves, let's fit `w` automatically. Much of this follows from the linear regression notebook: we're going to use gradient descent to minimise a loss function. This time, the loss function is the cross-entropy loss fuction."]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Create a function `cross_entropy`\n", "Which takes as input:\n", "* a vector y of true labels (either 0 or 1)\n", "* a vector y_pred of predicted probabilities (between 0 and 1)\n", "\n", "And outputs:\n", "* a single value loss\n", "\n", "Cross-entropy loss L is defined as:\n", "$$\n", "\\begin{align}\n", "L &= -\\frac{1}{N}\\sum_{n=1}^N\\log\\left( p_n^{y_n} (1-p_n)^{(1-y_n)}\\right) \\\\\n", "p_n &= y_\\text{pred} = \\sigma(x_n w)\n", "\\end{align}\n", "$$\n", "\n", "Dividing by N is a convention - multiplying by a constant doesn't change the shape of the function, so doesn't matter a great deal.\n", "\n", "Check it works by plotting the function when y=1 and varying y_pred between 0.1 and 0.9. What happens at y_pred=0?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def cross_entropy(y, y_pred):\n", "    preds = np.hstack([(1-y_pred), y_pred])\n", "    loss = -1/y.shape[0] * np.sum(np.log(preds[y.astype(int)]))\n", "    return loss"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# we provide the plotting code\n", "y_pred = np.linspace(.1, .9, 100)\n", "losses = np.zeros(y_pred.shape)\n", "for ii in range(y_pred.shape[0]):\n", "    losses[ii] = cross_entropy(np.array([1]), y_pred[ii])\n", "plt.plot(y_pred, losses)\n", "plt.xlabel('$y_{pred}$')\n", "plt.ylabel('$L$')\n", "plt.title('Cross-entropy loss for y=1 and $y_{pred}$')\n", "plt.grid(True)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Fitting `w` using cross entropy"]}, {"cell_type": "markdown", "metadata": {}, "source": ["##### Step 1: Initialise\n", "1. First initialise $w$ to any array of size $2$.\n", "\n", "1. We have defined $X$ above"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["w = ...\n", "\n", "w = np.array([0., 0.])\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["##### Step 2: Predicted Values\n", "Create a function that outputs the predicted values $\\hat{y}$ given the current value of $w$.\n", "\n", "Recall that $\\hat{y} = \\sigma(Xw)$\n", "\n", "*Hint: you've already done this above!*"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def predicted_values(w, X):\n", "    # calculate y_pred\n", "    a = X @ w\n", "    y_pred = sig(a)\n", "    return y_pred"]}, {"cell_type": "markdown", "metadata": {}, "source": ["##### Step 3: Loss Function\n", "Create a function that calulates the current loss of our predictions $\\hat{y} = \\sigma(Xw)$ using the cross-entropy loss.\n", "\n", "*Hint:* you've done all the work for this above!"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def loss_function(w, X, y):\n", "    # calculate y_pred using predicted_values\n", "    y_pred = predicted_values(w, X)\n", "    \n", "    \n", "    # calculate the cross-entropy loss from y and y_pred\n", "    loss = cross_entropy(y, y_pred)\n", "    \n", "    return loss"]}, {"cell_type": "markdown", "metadata": {}, "source": ["##### Step 4: Gradient\n", "Recall that the gradient of our loss function is (after some algebraic manipulation):\n", "\n", "$$\\nabla L(\\boldsymbol{w})  \\quad\\!\\!=\\quad\\!\\! \\frac{1}{N}\\boldsymbol{X}^T (\\hat{\\boldsymbol{y}} - \\boldsymbol{y})$$\n", "\n", "Create a function that returns the gradient for a given value of $w$ (and $X$ and $y$).\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def gradient(w, X, y):\n", "    ...\n", "    y_pred = predicted_values(w, X)\n", "    N = y.shape[0]\n", "    grad = 1/N * X.T @ (y_pred - y)\n", "    return grad"]}, {"cell_type": "markdown", "metadata": {}, "source": ["##### Step 5: Optimise!\n", "Let's again use our trust SimpleGD function from Module 1, perform 10000 steps of gradient descent and plot the evolution of the function values returned. This is identical to Linear Regression from here - see if you can remember how to code it up without peeking!\n", "\n", "We recommend using small step size to begin with: try $\\gamma = 10^{-2}$.\n", "\n", "Hint: if your loss increases towards $\\infty$, try resetting $w$ and reducing the stepsize."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Gradient descent function\n", "def simpleGD(w0, f, g, gamma, nr_steps):\n", "    history = np.zeros(nr_steps+1)\n", "    history[0] = f(w0)\n", "    w = w0\n", "    for ii in range(nr_steps):\n", "        # this formulation amounts to writing w = w - stepsize*g(w)\n", "        w -= gamma*g(w)\n", "        history[ii+1] = f(w)\n", "    return w, history"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# create versions of your loss and gradient functions that only require `w` as an input\n", "# (hint: you might want to use a lambda function.)\n", "\n", "loss_Xy = lambda w: loss_function(w, X, y)\n", "gradient_Xy = lambda w: gradient(w, X, y)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Perform gradient descent on your loss function, \n", "# w, history = simpleGD(...\n", "# w = np.zeros(2)    # in case step size is too large and loss explodes\n", "w, history = simpleGD(w, loss_Xy, gradient_Xy, 1e-2, 10000)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["##### Step 6: Optimisation Diagnostics\n", "Plot the history of your loss over your iterates. \n", "* Does it decrease or increase? \n", "    * If increasing, *reset your $w$* and optimize again with a smaller step size.\n", "    * If this doesn't fix the issue, you may have a bug in your code.\n", "* How quickly does it decrease?\n", "* Does it look like it's converging?\n", "* [**Question**] How might you speed up the convergence?\n", "\n", "Hint: be *very* careful when increasing the step size!"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot(history)\n", "plt.xlabel('iteration')\n", "plt.ylabel('loss')\n", "plt.show()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["##### Step 7: Plot the fit!\n", "\n", "You've fitted a the logistic regression parameters `w` using gradient descent!!!\n", "\n", "Why don't you treat yourself and plot the fit using `plot_logistic_probs()` from above"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["w"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plot_logistic_probs(w, X, y)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Bonus: Fit `w` with a bias term!"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can prepend a column of zeros to the data, much like for linear regression. The crossentropy loss gradient stays the same. Revisit the linear_regression notebook for hints."]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Edit your code to fit `w` but include a bias as the first term\n", "You will find that plot_logistic_probs will take a `w` of size 3: it expects the first term (i.e. `w[0]`) to be the bias."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["w = np.zeros(3)\n", "X = np.hstack([np.ones((X.shape[0], 1)), X])\n", "w, history = simpleGD(w, loss_Xy, gradient_Xy, 1e-2, 10000)\n", "plt.plot(history)\n", "plt.xlabel('iteration')\n", "plt.ylabel('loss')\n", "plt.show()\n", "print(w)\n", "plot_logistic_probs(w, X, y)\n"]}]}