{"metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.6"}}, "nbformat": 4, "nbformat_minor": 2, "cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Evaluation Metrics"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# import modules here\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Import the data\n", "We're using the flights dataset from before"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data = pd.read_csv('data/flights08_clean.csv')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Classification metrics"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Run the code below to perform a train test split, and fit a 5-nearest neighbor classifier."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train, X_test, y_train, y_test = train_test_split(data.drop('MajorDelay', axis=1), \n", "                                                    data['MajorDelay'], \n", "                                                    test_size=0.3, \n", "                                                    random_state=1337,\n", "                                                    stratify=data['MajorDelay'])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["knn = KNeighborsClassifier(n_neighbors=5)\n", "knn.fit(X_train, y_train)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Get y_hat - the classification predictions"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y_hat = ...\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Calculate the accuracy of the predictions"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["accuracy = (y_hat==y_test).mean()\n", "accuracy"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Why is this a bad metric for this data? Calculate the accuracy for each class. Then get the mean class accuracy."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Calculate the confusion matrix using sklearn.metrics.confusion_matrix. Use the given code to plot it, but make sure you label the rows and columns correctly..."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["cm = ...\n", "sns.heatmap(cm, annot=True, fmt='d')\n", "plt.xlabel(...)\n", "plt.ylabel(...)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Get the number of:\n", "* true positives\n", "* false positives\n", "* true negatives\n", "* and false negatives\n", "\n", "and arrange them in a matrix to match the confusion matrix above"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["nr_true_positives = ...\n", "nr_false_positives = ...\n", "nr_true_negatives = ...\n", "nr_false_negatives = ...\n", "\n", "\n", "manual_cm = np.array([\n", "    [..., ...],\n", "    [..., ...]\n", "])\n", "\n", "sns.heatmap(manual_cm, annot=True, fmt='d')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Get the precision, recall, and f1 score"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Check the values against sklearn.metrics.classification_report"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## Assessing probability outputs"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### ROC curves"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The metric \"Area Under the ROC Curve\", or AUC, doesn't care about how 'accurate' the probability is, only the ordering of the probabilities. If, when you order your data by the predicted probability and look at their true classes, a perfect ordering is when the classes are split perfectly. For example:\n", "\n", "This data is perfectly ordered\n", "```\n", "class   0,   0,   1,   1\n", "prob   .1,  .1,  .9, .99\n", "```\n", "\n", "As is this --\n", "```\n", "class   0,   0,   1,   1\n", "prob  .01,  .1, .11, .99\n", "```\n", "\n", "Whilst the first has 'better' probabilities, for both of these classifiers (that produced the data above), there exists a threshold which you could use to get perfect classification. They will both have the same ROC curve."]}, {"cell_type": "markdown", "metadata": {}, "source": ["The ROC curve plots the true positive rate (yet another name for recall or sensitivity i.e. nr_true_positives / nr_positive_cases) against the false_positive_rate (nr_false_positives / nr_negative_cases). A line is drawn by plotting the chages as the threshold moves from 1 (everything predicted negative - bottom left corner) to 0 (everything predicted positive - top right corner).\n", "\n", "Here's an example from wikipedia:\n", "\n", "![img](https://upload.wikimedia.org/wikipedia/commons/6/6b/Roccurves.png)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Write a function to get the points on an ROC curve\n", "The function should take as input:\n", "* The true labels y\n", "* The predicted probabilities y_pred\n", "\n", "and outputs:\n", "* fpr_array - a numpy array of false positive rates\n", "* tpr_array - the corresponding array of true positive rates\n", "* thresholds - the corresponding array of thresholds that obtained that tpr, fpr pair\n", "\n", "This is quite tricky, so we provide hints in the function skeleton below."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def get_fpr_tpr(y, y_pred, thresh):\n", "    # Write a function which gets the false positive rate\n", "    # and true positive rate when given a threshold\n", "    tpr = ...\n", "    fpr = ...\n", "    return fpr, tpr\n", "\n", "\n", "def my_roc_curve(y, y_pred):\n", "    # we need only check a finite number of thresholds\n", "    # get the unique values in y_pred and sort them\n", "    thresh = ...\n", "    \n", "    \n", "    # Loop through all the thresholds and record the \n", "    # fpr and tpr in an array\n", "    return fpr_array, tpr_array, thresh"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# we can now test this for our knn and plot\n", "y_pred = knn.predict_proba(X_test)[:,1]\n", "fpr, tpr, thresh = my_roc_curve(y_test, y_pred)\n", "plt.plot(np.append(fpr, 0), np.append(tpr, 0), 'o-')\n", "plt.xlabel('false positive rate')\n", "plt.ylabel('true positive rate')\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# And we can check it against the sklearn implementation\n", "from sklearn.metrics import roc_curve\n", "fpr, tpr, thresh = roc_curve(y_test, y_pred)\n", "plt.plot(fpr, tpr, 'o-')\n", "plt.xlabel('false positive rate')\n", "plt.ylabel('true positive rate')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Get the AUC with sklearn.metrics.roc_auc_score\n", "The AUC is simply the area under this curve, the larger the better. The best possible scenario is that you have an AUC of 1 which means there is a point in the top left corner i.e. tpr=1 and fpr=0."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Extension: Fit a Logistic Regression and a Decision Tree Classifier and compare their ROC curves and AUC\n", "A couple of things to think about:\n", "* Which classifier is best and why?\n", "* Why does knn only have a few points whereas logistic has many?\n", "* Why does a decision tree to max depth only have one point?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Where to next? Try the [brier score](http://scikit-learn.org/stable/modules/model_evaluation.html#brier-score-loss) - This aims to measure classification probability quality, not simply ordering. \n", "\n", "For regression, see the [r2_score](http://scikit-learn.org/stable/modules/model_evaluation.html#r2-score-the-coefficient-of-determination)."]}]}