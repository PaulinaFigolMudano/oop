{"metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.5"}}, "nbformat": 4, "nbformat_minor": 2, "cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Object Oriented Programming"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In this practical, we will implement our own models following the conventions from scikit-learn and test them on a real dataset. \n", "\n", "Since we want to write production code, we will work a bit differently than usual, leveraging the pros of a good IDE and a Jupyter Notebooks.\n", "\n", "\n", "**[Start opiniated comment]**\n", "\n", "\n", "Notebooks are great for prototyping models and work interactively with your data. They aren't great for code that you want to maintain and use in production (you cannot import from a notebook, they're hard to version control, hard to test, etc.. which are fundamental things in software engineering). Hence, when you're \"happy\" with a model you have prototyped in a notebook, it is important to implement it as proper Python code in a `.py` file before distributing it.\n", "\n", "**[End opiniated comment]**\n", "    \n", "There is a great functionality on notebooks that allows us to develop production code in a `.py` file and test it in the notebook, it's called `autoreload`: you can import functions and classes from your `.py` file, and use them in the notebook. If you modify the file, it will automatically reload it in the notebook.\n", "\n", "That's what we will do here, we will develop our own library, called `not_sklearn` that will contain two models, a simple one (called `SimpleModel` and another one for logistic regression. We will import the models in this notebook and test them whilst we develop them.\n", "\n", "\n", "*Note: If you don't feel comfortable working in both an IDE and notebook, try it. This is important, Data Scientists are more and more expected to write production ready code, whilst some companies still accept a notebook as deliverable, your software engineer colleagues will be gratefull if you can deliver nicely packaged code instead. If you find it really hard, you can copy paste the class from the `.py` file into this notebook and work on it here.*"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# That is setting up the autoreload mode to automatically \n", "# reload your imports when you edit a file\n", "%load_ext autoreload\n", "%autoreload 2\n", "\n", "import numpy as np"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Here we only need a toy dataset to test our implementation, we'll use the sklearn `load_breast_cancer` function to directly load the Breast Cancer dataset as numpy arrays. By setting `return_X_y` to `True` we get both `X` and `y` which will be the inputs for out `fit` methods"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.datasets import load_breast_cancer\n", "X, y = load_breast_cancer(return_X_y=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We also need some data to predict on, so we'll use `train_test_split` to split our data into training and test sets."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Implement a model that predicts the most common class everytime"]}, {"cell_type": "markdown", "metadata": {}, "source": ["First we will implement a really simple model that learns what the most common class from the training set, saves it as an internal state and predicts everything as this class.\n", "\n", "For instance, if we have the following training data:\n", "```\n", "X = [[...]] # X doesn't matter here, we will only learn from the target\n", "y = [\"cat\", \"dog\", \"cat\"]\n", "```\n", "\n", "We'll learn to predict `cat` everytime. \n", "\n", "When we call predict with say 5 observations we will return 5 predictions, each one being `cat`.\n", "\n", "For our model to be compatible with the `sklearn` ecosystem, we will make sure to follow the same conventions, and call our method that train the algorithm `fit` (which takes X and y as inputs) and the method that generates predictions given an X will be `predict`"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# To load the solution comment the first line and uncomment the second\n", "from not_sklearn import SimpleModel\n", "# from not_sklearn_solutions import SimpleModel"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Implement the `__init__` method in your `SimpleModel` class and create a new instance below:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Check the attribute `most_common` (you'll need to print it to verify that it's properly set to `None`)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Now implement the `fit` method in the file, fit takes two arguments: `X` and `y` and will train your model. \n", "For our `SimpleModel`, training means finding the most common class in `y` and saving it as our `most_common` attribute.\n", "\n", "Call `fit` from your model passing `X_train` and `y_train` as arguments:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Check the attribute `most_common` again, if you implemented `fit` properly, it should correspond to the majority class in the `y` that was provided (here `y_train`)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Perfect, we're almost done. Only the `predict` method to implement. Here we take an `X` as data to predict, and for each observation in `X` we want to predict the same value, our most commong class `most_common`. Implement it in the file and test it with `x_test` below and save the result as `y_pred`"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Make sure that `y_pred` has the same dimension as `y_test`"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Great, now load the accuracy score function from `sklearn.metrics` and check the accuracy of your model on the training set:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Well done on implementing your first own model! Now let's make it more challenging and implement a proper algorithm, logistic regression!"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Implement the logistic regression algorithm"]}, {"cell_type": "markdown", "metadata": {}, "source": ["For this part it is recommended to open the logistic regression notebook, we will follow the same process but implement our own `LogisticRegression` class."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# To load the solution comment the first line and uncomment the second\n", "from not_sklearn import LogisticRegression\n", "# from not_sklearn_solutions import LogisticRegression"]}, {"cell_type": "markdown", "metadata": {}, "source": ["First we need to implement the `__init__` method that will initialise our `LogisticRegression`. Remember that we will need a set of weights `w`, since we do not know yet the dimension of the data we will train our model on, we can only initialise `w` to `None`. We also need to add two parameters that control our algorithm, the step size `gamma` and number of iterations `nr_steps`.\n", "\n", "Once you have implemented your `__init__`, instanciate your model with the default parameters and check that the attributes were properly initialised."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Before being able to implement our `fit` and `predict` methods, we will need a few helper methods:\n", "- `sigmoid`: that computes the sigmoid of any vector\n", "- `predicted_values`: similar to the one we did in the logistic regression notebook, it will allow us to compute predictions for a given `X` and `w`.\n", "- `gradient`: that computes the gradient of our loss function given `X`, `y` and `w`. This is the same as we did in the logistic regression notebooks."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Implement the `sigmoid` method that takes an `X` and compute its sigmoid (it's the same as we did previously, just that now its a method on the class, so its first argument will be `self`, and second `X`).\n", "\n", "Try to call your sigmoid method on some numpy vector of your choice."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Now implement the `predicted_values` method which takes as input:\n", "* the data `X` - an NxD dimensional matrix of the data (N datapoints)\n", "* a vector `w` - a D dimensional vector of the parameters\n", "\n", "and returns:\n", "* `p` - an N dimensional output of predicted probabilities\n", "\n", "(you can use your `sigmoid` method)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You can test your method using `X_train` as X matrix and the following vector for weights:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["w = np.zeros(X_train.shape[1])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Add your code here\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Finally, implement the `gradient` method that computes the gradient of the cross entropy loss, it takes:\n", "* `X` - an NxD dimensional matrix of the data (N datapoints)\n", "* `y` - a N dimensional vector containing the true labels\n", "* `w` - a D dimensional vector of the parameters\n", "\n", "and returns:\n", "* a vector, the gradient of the cross entropy loss.\n", "\n", "It's the same as for the logistic regression notebook, just make sure you're reusing the predicted_values method we've defined on the class."]}, {"cell_type": "markdown", "metadata": {}, "source": ["You can test your function in the notebook by calling it with `X_train`, `y_train` and the same set of weights as before:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Great, now we're fully equiped to write the `fit` function.\n", "Implement the fit function that takes a matrix X and a vector y as parameters.\n", "It should initialise self.w to a vector of zeros with the right dimension, \n", "\n", "Then make self.nr_steps iterations of gradient descent to update self.w\n", "\n", "It is similar to what the simpleGD function from the previous notebook does,\n", "but here we do not need to keep the history, only setting w."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Try your `fit` function on `X_train` and `y_train`, and check how the attribute `w` has changed:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Great, now we can implement our last method, `predict`.\n", "It takes a matrix X as input and returns `y_pred` a vector of 0s and 1s. \n", "It needs to use self.predicted value with X and the current set of weights, self.w. Check if the probability is higher than a given threshold (use 0.5) and return the right classes accordingly."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Test your predict method on `X_train` and save the result in a variable `y_pred`"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Calculate the accuracy of your model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}]}